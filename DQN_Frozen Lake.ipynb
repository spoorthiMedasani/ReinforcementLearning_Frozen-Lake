{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "Map= 'SFFFHFFFFFFFFFFG'\n",
    "env_name= \"FrozenLake-v0\"\n",
    "amap=np.asarray(Map, dtype='c')\n",
    "shape=int(np.sqrt(amap.shape[0]))\n",
    "amap=amap.reshape(shape,shape)\n",
    "env=gym.make(env_name, desc=amap).unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenLake():\n",
    "    \n",
    "    def __init__(self, env, discount_rate= 1.0, learning_rate= 0.25):     \n",
    "        self.state_space  = env.observation_space.n\n",
    "        self.action_space = env.action_space.n\n",
    "        self.discount_rate= discount_rate\n",
    "        self.learning_rate= learning_rate\n",
    "        self.eps= 1.0\n",
    "        self.build_model()\n",
    "        \n",
    "        self.sess= tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def build_model (self):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.state= tf.placeholder(tf.int32, shape= [1])\n",
    "        self.action= tf.placeholder(tf.int32, shape= [1])\n",
    "        self.target= tf.placeholder(tf.float32, shape= [1])\n",
    "        \n",
    "        self.state_b= tf.one_hot(self.state, depth=self.state_space)\n",
    "        self.action_b= tf.one_hot(self.action, depth=self.action_space)\n",
    "        \n",
    "        self.q_state_1= tf.layers.dense(self.state_b, units=100, name=\"q_table_1\")\n",
    "        #self.q_state_2= tf.layers.dense(self.q_state_1, units=50, name=\"q_table_2\")\n",
    "        self.q_state_3= tf.layers.dense(self.q_state_1, units=self.action_space, name=\"q_table_3\")\n",
    "        self.q_value= tf.reduce_sum(tf.multiply(self.q_state_3, self.action_b), axis=1)\n",
    "        \n",
    "        self.loss= tf.reduce_sum(tf.square(self.target - self.q_value))\n",
    "        self.optimizer= tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "    def get_action(self, cur_state):\n",
    "        q_value= self.sess.run(self.q_state_3, feed_dict= {self.state: [cur_state]})\n",
    "        \n",
    "        greedy= np.argmax(q_value)\n",
    "        random= np.random.randint(self.action_space)\n",
    "\n",
    "        if np.random.random() < self.eps:\n",
    "            return(random)\n",
    "        else:\n",
    "            return(greedy)\n",
    "        \n",
    "    def get_action_test(self, cur_state):\n",
    "        q_value= self.sess.run(self.q_state_3, feed_dict= {self.state: [cur_state]})\n",
    "        \n",
    "        return(np.argmax(q_value))\n",
    "    \n",
    "    def train (self, experience):\n",
    "        cur_state, action, next_state, reward, done= experience\n",
    "        \n",
    "        if experience[4]:\n",
    "            target=0\n",
    "            self.eps= self.eps*0.99\n",
    "        else:\n",
    "            q_next= self.sess.run(self.q_state_3, feed_dict={self.state: [next_state]})\n",
    "            target = reward + self.discount_rate*np.max(q_next)\n",
    "            \n",
    "        self.sess.run(self.optimizer, feed_dict= {self.state: [cur_state], self.action: [action], self.target: [target]})\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.sess.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent= FrozenLake(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04904089407128576\n",
      "  (Up)\n",
      "SFFF\n",
      "HFFF\n",
      "FFFF\n",
      "FFF\u001b[41mG\u001b[0m\n",
      "35.0\n"
     ]
    }
   ],
   "source": [
    "Total_reward=0\n",
    "\n",
    "for i in range(100):\n",
    "    cur_state= env.reset()\n",
    "    \n",
    "    done= False\n",
    "    while not done:\n",
    "        action = agent.get_action(cur_state)\n",
    "        next_state, reward, done, info= env.step(action)\n",
    "\n",
    "        agent.train((cur_state, action, next_state, reward, done))\n",
    "        Total_reward+=reward\n",
    "        \n",
    "        cur_state= next_state\n",
    "    \n",
    "        print(agent.eps)\n",
    "        '''\n",
    "        with tf.variable_scope(\"q_table\",reuse= True):\n",
    "            weights= agent.sess.run(tf.get_variable(\"kernel\"))\n",
    "            print(weights)\n",
    "        '''\n",
    "\n",
    "        env.render()\n",
    "        print(Total_reward)\n",
    "        #time.sleep(0.3)\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
