{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spoorthimedasani/miniconda3/envs/AV/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/spoorthimedasani/miniconda3/envs/AV/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/spoorthimedasani/miniconda3/envs/AV/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/spoorthimedasani/miniconda3/envs/AV/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/spoorthimedasani/miniconda3/envs/AV/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/spoorthimedasani/miniconda3/envs/AV/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Map= 'SFFFHFFFFFFFFFFG'\n",
    "env_name= \"FrozenLake-v0\"\n",
    "amap=np.asarray(Map, dtype='c')\n",
    "shape=int(np.sqrt(amap.shape[0]))\n",
    "amap=amap.reshape(shape,shape)\n",
    "env=gym.make(env_name, desc=amap).unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-2e4412174239>:24: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /Users/spoorthimedasani/miniconda3/envs/AV/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/spoorthimedasani/miniconda3/envs/AV/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "class FrozenLake():\n",
    "    \n",
    "    def __init__(self, env, discount_rate= 1.0, learning_rate= 0.25):     \n",
    "        self.state_space  = env.observation_space.n\n",
    "        self.action_space = env.action_space.n\n",
    "        self.discount_rate= discount_rate\n",
    "        self.learning_rate= learning_rate\n",
    "        self.eps= 1.0\n",
    "        self.build_model()\n",
    "        \n",
    "        self.sess= tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def build_model (self):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.state= tf.placeholder(tf.int32, shape= [1])\n",
    "        self.action= tf.placeholder(tf.int32, shape= [1])\n",
    "        self.target= tf.placeholder(tf.float32, shape= [1])\n",
    "        \n",
    "        self.state_b= tf.one_hot(self.state, depth=self.state_space)\n",
    "        self.action_b= tf.one_hot(self.action, depth=self.action_space)\n",
    "        \n",
    "        self.q_state= tf.layers.dense(self.state_b, units=self.action_space, name=\"q_table\")\n",
    "        self.q_value= tf.reduce_sum(tf.multiply(self.q_state, self.action_b), axis=1)\n",
    "        \n",
    "        self.loss= tf.reduce_sum(tf.square(self.target - self.q_value))\n",
    "        self.optimizer= tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "    def get_action(self, cur_state):\n",
    "        q_value= self.sess.run(self.q_state, feed_dict= {self.state: [cur_state]})\n",
    "        \n",
    "        greedy= np.argmax(q_value)\n",
    "        random= np.random.randint(self.action_space)\n",
    "\n",
    "        if np.random.random() < self.eps:\n",
    "            return(random)\n",
    "        else:\n",
    "            return(greedy)\n",
    "    \n",
    "    def train (self, experience):\n",
    "        cur_state, action, next_state, reward, done= experience\n",
    "        \n",
    "        if experience[4]:\n",
    "            target=0\n",
    "            self.eps= self.eps*0.99\n",
    "        else:\n",
    "            q_next= self.sess.run(self.q_state, feed_dict={self.state: [next_state]})\n",
    "            target = reward + self.discount_rate*np.max(q_next)\n",
    "            \n",
    "        self.sess.run(self.optimizer, feed_dict= {self.state: [cur_state], self.action: [action], self.target: [target]})\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.sess.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent= FrozenLake(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04904089407128576\n",
      "[[ 1.39850521e+01  3.65275040e+01  3.93565903e+01  1.28192184e+02]\n",
      " [ 1.29851242e+02  1.47784378e+02  1.44239883e+02  1.30923798e+02]\n",
      " [ 1.37507828e+02  1.47232590e+02  1.46830338e+02  1.26600571e+02]\n",
      " [ 1.36428177e+02  1.48665955e+02  1.48389175e+02  1.26093880e+02]\n",
      " [ 9.88352299e-02  4.18891191e-01  5.35588622e-01 -2.54441500e-02]\n",
      " [ 6.77535095e+01  6.21759176e+00  1.47164505e+02  3.01646767e+01]\n",
      " [ 1.31871399e+02  1.39454391e+02  1.43808258e+02  1.27759476e+02]\n",
      " [ 1.34973846e+02  1.27332611e+02  1.46624512e+02  1.27776581e+02]\n",
      " [ 3.50320396e+01  1.42183350e+02  2.24239273e+01 -2.43800163e+01]\n",
      " [ 1.00050415e+02  1.24884964e+02  1.47258102e+02  1.20557541e+02]\n",
      " [ 1.15764038e+02  1.33322632e+02  1.41880600e+02  1.27798943e+02]\n",
      " [ 6.86162491e+01  4.31229324e+01  3.24232445e+01  1.27853355e+02]\n",
      " [ 3.52605553e+01  3.95696983e+01  1.24579636e+02  1.72731037e+01]\n",
      " [ 8.36522293e+01  1.00395988e+02  1.46731522e+02  1.12733154e+02]\n",
      " [ 1.38924210e+02  5.19734726e+01  5.37920456e+01  2.56105556e+01]\n",
      " [-5.90063632e-02 -4.77456331e-01 -1.43418223e-01 -2.96044260e-01]]\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mH\u001b[0mFFF\n",
      "FFFF\n",
      "FFFG\n",
      "26.0\n"
     ]
    }
   ],
   "source": [
    "Total_reward=0\n",
    "\n",
    "for _ in range(100):\n",
    "    cur_state= env.reset()\n",
    "    \n",
    "    done= False\n",
    "    while not done:\n",
    "        action = agent.get_action(cur_state)\n",
    "        next_state, reward, done, info= env.step(action)\n",
    "\n",
    "        agent.train((cur_state, action, next_state, reward, done))\n",
    "        Total_reward+=reward\n",
    "        \n",
    "        cur_state= next_state\n",
    "    \n",
    "        print(agent.eps)\n",
    "        with tf.variable_scope(\"q_table\",reuse= True):\n",
    "            weights= agent.sess.run(tf.get_variable(\"kernel\"))\n",
    "            print(weights)\n",
    "\n",
    "        env.render()\n",
    "        print(Total_reward)\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
